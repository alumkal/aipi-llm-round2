{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对这个任务，我有一处疑问：为什么 $\\mathrm{embed\\_size}$ 必须是 $\\mathrm{num\\_heads}$ 的整数倍？按照我的理解，$\\mathrm{embed\\_size}$、$\\mathrm{dim\\_qk}$、$\\mathrm{dim\\_v}$ 和 $\\mathrm{num\\_heads}$ 应该都是可自由调整的超参数，原文取 $\\mathrm{dim\\_qk = dim\\_v = embed\\_size / num\\_heads}$ 只是出于方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(114514)\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # 采用最直接的实现方式；实际应用需要考虑数值稳定性\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None, verbose=False):\n",
    "    assert mask is None\n",
    "    if verbose:\n",
    "        print(f'QKV.shape={Q.shape}')\n",
    "    dim_qkv = Q.shape[-1]\n",
    "    attn_score = (Q @ K.swapaxes(-1, -2)) / np.sqrt(dim_qkv)\n",
    "    attn_weights = softmax(attn_score)\n",
    "    output = attn_weights @ V\n",
    "    if verbose:\n",
    "        print(f'{output.shape=} {attn_weights.shape=}')\n",
    "    return output, attn_weights\n",
    "\n",
    "def multi_head_attention(embed_size, num_heads, input, mask=None, verbose=False):\n",
    "    # 假设 dim_qk = dim_v = embed_size / num_heads\n",
    "    # 本实现不考虑 QKV 的 bias 项\n",
    "    assert embed_size % num_heads == 0\n",
    "    dim_qkv = embed_size // num_heads\n",
    "    Wq = np.random.randn(num_heads, embed_size, dim_qkv)\n",
    "    Wk = np.random.randn(num_heads, embed_size, dim_qkv)\n",
    "    Wv = np.random.randn(num_heads, embed_size, dim_qkv)\n",
    "    Wo = np.random.randn(embed_size, embed_size)\n",
    "    Bo = np.zeros(embed_size)\n",
    "    if verbose:\n",
    "        print(f'W_qkv.shape={Wq.shape}')\n",
    "        print(f'{Wo.shape=} {Bo.shape=}')\n",
    "    input = input[:, None, :, :]\n",
    "    if verbose:\n",
    "        print(f'{input.shape=}')\n",
    "    output, weights = scaled_dot_product_attention(\n",
    "        input @ Wq, input @ Wk, input @ Wv, mask, verbose)\n",
    "    output = output.swapaxes(-2, -3)\n",
    "    output = output.reshape([*output.shape[:-2], -1])\n",
    "    output = output @ Wo + Bo\n",
    "    if verbose:\n",
    "        print(f'{output.shape=} {weights.shape=}')\n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试 `multi_head_attention` 的功能。输入的 batch 大小为 3，序列长度为 11；模型共 5 个 head，每个 head 的 qkv 大小为 7。选取这几个数是为了方便观察 shape 的变化规律。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_qkv.shape=(5, 35, 7)\n",
      "Wo.shape=(35, 35) Bo.shape=(35,)\n",
      "input.shape=(3, 1, 11, 35)\n",
      "QKV.shape=(3, 5, 11, 7)\n",
      "output.shape=(3, 5, 11, 7) attn_weights.shape=(3, 5, 11, 11)\n",
      "output.shape=(3, 11, 35) weights.shape=(3, 5, 11, 11)\n",
      "[ -1.00275258 -25.66227608  42.57650594   7.97341477  -2.09239899\n",
      "  22.53574569 -32.31421119 -19.31954746  35.94738272   5.09795971\n",
      " -34.47604002   0.86513501  50.51554347  21.8124433   35.35536458\n",
      " -30.79651531   0.38839876   6.82163086 -14.5239423  -50.32858852\n",
      "  20.92636831 -11.40505511  34.35585814  -8.64440007  17.03970826\n",
      " -46.23846407   0.86446847  27.91816735  -6.19561116 -11.2085796\n",
      "  -0.52242257 -86.61101946 -23.54598171 -26.04331552 -26.03110728]\n",
      "[1.29420131e-12 1.81028363e-33 4.99676145e-31 5.48498138e-21\n",
      " 3.03060036e-26 1.09915871e-16 3.71961110e-10 1.56721677e-26\n",
      " 1.97962592e-25 1.00000000e+00 2.35854129e-25]\n"
     ]
    }
   ],
   "source": [
    "embed_size = 5*7\n",
    "num_heads = 5\n",
    "input = np.random.randn(3, 11, embed_size)\n",
    "output, weights = multi_head_attention(embed_size, num_heads, input, verbose=True)\n",
    "print(output[0, 0])\n",
    "print(weights[0, 0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
